**************************************获取整个网页*************************************************
from urllib.request import urlopen                  //引入模块（url.request）中的urlopen对象
html = urlopen("http://https://github.com/yyt-for-work/Practice-in-leetcode")   //根据网址将网页解析为html文件
print(html.read())				//打印html文件


上面代码通过url信息将整个网页搞了下来，但是我们的需要的信息包含在网页中，而不是整个网页。
******************************开始学习BeautifulSoup*************************************************
8.8今天学习使用BeautifulSoup：不是Python的标准库，因此需要单独安装。
from urllib.request import urlopen
from bs4 import BeautifulSoup		//将BeautifulSoup对象引入
html = urlopen("https://github.com/yyt-for-work/Practice-in-leetcode")
bsobj = BeautifulSoup（html.read()）       //使用BeautifulSoup将网页信息进行解析
print（bsobj.h1）			//解构后数据访问的范围到达标签级。


上面是BeautifulSoup的基本使用方法，BeautifulSoup的基本功能。
***************************8.8讨论scrapy在工作时可能会遇到的异常情况*****************************************

动作一：html = urlopen("https://www.github.com/yyt-for-work/Practice-in-leetcode")
通过url得到网页的源代码
可能的异常：1.网页在服务器上不存在。-----urlopen函数会抛出“HTTPError”异常。
解决方法：使用try【可能出错代码】except【捕获异常并进行处理】
	2.URL链接不能使用。----urlopen函数返回一个None对象----解决方法：if  html is None:   处理  else：程序继续
动作二：bsobj.h1.span   通过bs对象调用一个标签的子标签。
可能的异常：1.父标签不存在，子标签更不会存在。----2.bs对象返回一个AttributeError----3.解决方法：使用try----except


总结：在动作一处使用try+if(能否得到html对象，得到的html对象的内容是否为none)处理异常；在动作二处使用if来避免异常。
经典防错写法：
from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup
	def getTitle(url):
		try:
			html = urlopen(url)
		except HTTPError as e:
			return None
		try:
			bsobj = BeautifulSoup(html.read())
			title = bsobj.body.h1
		except AttributeError as e:
			return None
		return title
	title = getTitle("http://www.pythonscraping.com/pages/page1.html")
	if title ==None:
		print("Title could not be found")
	else:
		print(title)

第一章内容结束
**************************************第一章内容总结************************************************************
1.学习了urllib库中的urllib.request模块中的urlopen方法。
2.学习了urllib库中的urllib.error模块中的HTTPError方法。
3.学习使用bs4库中BeautifulSoup对象，将整个的html文件分解为各个标签。
4.学习在爬虫运行时可能会出现的错误，并且使用各种代码规避错误。
***********************************第二章   复杂的HTML解析******************************************************
本章主要讨论的问题是：在HTML的烂泥堆中提取有用的数据。
本章内容的含义：在上一章中我们使用BeautifulSoup对象将HTML文件分割为一个个小标签，而本章内容介绍通过标签中的属性来筛选出我们需要的标签。
2.1写代码前的思考
温馨小提示：面对埋藏很深或格式不友好的数据时，不要不经思考就写代码，一定要想好数据获取路径。
********************************本节开始筛选不同种类的标签*******************************************************
使用了findAll()函数，find（）函数，导航树来完成标签的选择
2.2使用BeautifulSoup

	使用BeautifulSoup通过属性查找标签的方法
		findAll()的使用：锁定标签类型--->锁定属性名--->锁定属性值
		findAll（“标签名”，{“属性名”：“属性值”}）  官方写法为：findAll（tagName，tagAttributes，recursive，text，limit，keywords）
		但是在95%的时间里都只需要前两个参数。详情请查询p33
		find()的使用：同findAll（），但是最后得到的是一个标签而不是一个标签组。
	番外篇：get_text()函数的使用
		谨慎使用get_text(),因为此函数会将所有的标签都清除掉。
		使用findAll（）函数得到一堆的标签时，我们并没有得到我们最终想要的数据
		此时使用  标签.get_text()  我们将得到标签中的文本信息，而将标签信息清除。
	使用BeautifulSoup中的导航树通过标签在文档中的位置来查找标签
		思想：BeautifulSoup在解析HTML时将一个标签的所有与他存在关系的
		标签划分为四种后代标签，子标签，兄弟标签，父标签。通过一个标签的标签关系得到的
		标签便是通过标签在文档中的位置来查找标签。
		一个HTML文件可以映射成一棵树，HTML标签就是根，body，head就是第一层子节点。
		使用子标签与后代标签：标签.children   得到标签的所有子标签
				标签.descendants()得到标签的所有子标签，与子标签的子标签。。。
		使用兄弟标签：标签.next_siblings   这个函数只调用标签后面的兄弟标签
			       标签.previous_siblings   这个函数只会调用标签前面的兄弟标签。
		使用父标签：标签.parent   这个函数会返回父标签。   父标签使用非常少。
******************************************************8.14正则表达式的使用***************************************************
使用正则表达式是为了完成文本内容的筛选。
1.看懂正则表达式
	正则表达式中的每个字符之间都是且的关系。
	适配符与它前面的一个字符或者第一个括号组合使用。
	十二个适配符的含义：
		（）：括号内的内容为一个整体。
		^：指字符串开始位置的东西。^a：字符串的开头必须是a。
		$：出现在正则表达式的末尾，表示匹配动作由字符串的末端开始，即先查看字符串末端的字符是否符合正则表达式的规定。
		.:匹配任意单个字符。
		{m,n}:匹配前面的东西m到n次。
		[]:匹配[]中的任意一个字符。
		+：匹配前面的东西至少一次。
		*：匹配前面的东西0次或多次。
		
		[^]:匹配任意一个不在[^]中的字符。如[^A-C]，除了A，B，C都可以匹配。
		|：匹配任意一个由竖线分割的东西。如（a|b|c）。

		\（反斜杠）：转义字符，将特殊含义的字符转换成字面形式。
		?!：表示“不包含”的意思。使用时将?!放在一个东西前面，表示字符串中不能出现这个东西。
**************************************************BeautifulSoup和正则表达式的使用***********************************
BeautifulSoup总是和正则表达式联合使用。
它们两个是怎样联合使用的呢？
BeautifulSoup的工作是将HTML文件分解为标签，然后根据通过find（）findAll（）函数查找目标标签。
而正则表达式可以在find（），findAll（）函数的使用中作为属性值筛选时使用。
例如：
images = bsObj.findAll（“img”，{“src”:re.compile（“\.\.\./img\/gifts/img.*\.jpg”）}）
上面语句的含义是：寻找img标签，且src的属性值为../img/gifts/img(.*:表示任意多个字符).jpg
8.
终于要对属性下手了
Tag1.attrs["属性名1"]方法将会得到Tag1中属性1的属性值。
**************************BeautifulSoup+Lambda可替代B+正则表达式*************************************************

Lambda表达式的使用
Lambda表达式本质上是一个函数，可以作为其他函数的参数使用，比如f(g(x),y）,g(x)就是Lambda表达式。
Lambda表达式与findAll函数结合使用：
BeautifulSoup允许把特定的函数类型当作findAll（）函数的参数。
函数类型：必须把一个标签作为参数，返回结果必须是布尔值。
具体使用范例书上暂且没有。
**************************************************第二章内容结束后的总结**************************************************
第一章的内容主要是获取整个网页到python中，使用BeautifulSoup将HTML文件分解为各种小标签
第二章的内容为对各种小标签进行挑选，找出需要的小标签。
1.使用导航树进行挑选；2.使用find，findAll函数+正则表达式；3.使用find，findAll函数+Lambda。

*************************************************第三章  开始采集*********************************************************
本章web crawler开始沿着网络爬取数据而不是在一个页面转圈。

完成第一个网络爬虫的使用。开心

下面这个函数将会由一个网页沿着url跳到另一个网页，再沿着另一个url跳向另一个网页。
from urllib.request import urlopen
from bs4 import BeautifulSoup
import datetime
import random
import re

random.seed(datetime.datetime.now())
def getLinks(articleUrl):
	html = urlopen("http://en.wikipedia.org"+articleUrl)
	bsObj = BeautifulSoup(html)
	return bsObj.find("div",{"id":"bodyContent"}).findAll("a",
		href=re.compile("^(/wiki/)((?!:).)*$"))
links = getLinks("/wiki/Kevin_Bacon")                      /*得到一个数组的链接
while len(links)>0:
	newArticle = links[random.randint(0,len(links)-1)].attrs["href"]/*选一个新的链接进入
	print（newArticle）
	links = getLinks(newArticle)

********************************************对整个网站信息进行采集*****************************
网站：网站含义不同于一个网页，它包含多个网页信息。新浪，百度，维基等都是一个网站。
浅网（surface web）：使用互联网搜索引擎可以抓取的网站，占整个互联网比重较小。
深网（deep web）：占互联网比重的90%以上，不能被搜索引擎抓取到。
暗网（dark web）：与普通互联网网站在使用协议上有区别，在HTTP之上还有一层新协议，Tor客户端可以支持使用该协议。


怎样采集一个网站的信息：
逻辑上：由顶级页面开始（比如主页），采集页面上所有的url形成一个列表；通过采集到的url进入
另一个页面；再收集改页面上的所有的url，形成列表，如此重复递归。
为了避免一个页面被采集两次，链接去重是非常重要的。将已经发现的所有链接都放在一起，
并保存在set集合中，set可以方便快捷的查询一个元素是否已经被放入，

代码实现：
href = re.compile("^(/wiki/)"):       //该句代码含义为挑选开头为/wiki/的链接
/*可能需要改为

递归警告：
Python默认的递归限制（即函数调用自己本身的次数最多1000次），在达到一千次后程序会停止。
可以设置一个较大的递归计数器来解决问题。
********************************************收集整个网站数据******************************
此内容开始告别使用爬虫在网页之间无聊跳转，我们要开始使用爬虫做一些事情了。
任务：
创建一个爬虫来收集页面标题，正文的第一个段落，编辑页面链接。
第一步：先观察网页特征，然后拟定一个采集模式。
	大标题的位置：<h1 id="firstHeading" class="firstHeading" lang="en">Scottish Highlands</h1>
	正文第一段：<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div role="note" class="hatnote navigation-not-searchable">"Highland Line" redirects here. For the railway lines, see <a href="/wiki/Highland_Main_Line" title="Highland Main Line">Highland Main Line</a> and <a href="/wiki/West_Highland_Line" title="West Highland Line">West Highland Line</a>.</div>





		
		





		
		
	


