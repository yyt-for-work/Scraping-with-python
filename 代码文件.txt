from urllib.request import urlopen                  //引入模块（url.request）中的urlopen对象
html = urlopen("http://https://github.com/yyt-for-work/Practice-in-leetcode")   //根据网址将网页解析为html文件
print(html.read())				//打印html文件


上面代码通过url信息将整个网页搞了下来，但是我们的需要的信息包含在网页中，而不是整个网页。
**********************************************************************************************************
8.8今天学习使用BeautifulSoup：不是Python的标准库，因此需要单独安装。
from urllib.request import urlopen
from bs4 import BeautifulSoup		//将BeautifulSoup对象引入
html = urlopen("https://github.com/yyt-for-work/Practice-in-leetcode")
bsobj = BeautifulSoup（html.read()）       //使用BeautifulSoup将网页信息进行解析
print（bsobj.h1）			//解构后数据访问的范围到达标签级。


上面是BeautifulSoup的基本使用方法，BeautifulSoup的基本功能。
************************************************************************************************************
8.8讨论scrapy在工作时可能会遇到的异常情况

动作一：html = urlopen("https://www.github.com/yyt-for-work/Practice-in-leetcode")
通过url得到网页的源代码
可能的异常：1.网页在服务器上不存在。-----urlopen函数会抛出“HTTPError”异常。
解决方法：使用try【可能出错代码】except【捕获异常并进行处理】
	2.URL链接不能使用。----urlopen函数返回一个None对象----解决方法：if  html is None:   处理  else：程序继续
动作二：bsobj.h1.span   通过bs对象调用一个标签的子标签。
可能的异常：1.父标签不存在，子标签更不会存在。----2.bs对象返回一个AttributeError----3.解决方法：使用try----except


总结：在动作一处使用try+if(能否得到html对象，得到的html对象的内容是否为none)处理异常；在动作二处使用if来避免异常。
经典防错写法：
from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup
	def getTitle(url):
		try:
			html = urlopen(url)
		except HTTPError as e:
			return None
		try:
			bsobj = BeautifulSoup(html.read())
			title = bsobj.body.h1
		except AttributeError as e:
			return None
		return title
	title = getTitle("http://www.pythonscraping.com/pages/page1.html")
	if title ==None:
		print("Title could not be found")
	else:
		print(title)


**************************************************第一章结束******************************************
第二章   复杂的HTML解析
本章主要讨论的问题是：在HTML的烂泥堆中提取有用的数据。
本章内容的含义：在上一章中我们使用BeautifulSoup对象将HTML文件分割为一个个小标签，而本章内容介绍通过标签中的属性来筛选出我们需要的标签。


2.1写代码前的思考
温馨小提示：面对埋藏很深或格式不友好的数据时，不要不经思考就写代码，一定要想好数据获取路径。
 

2.2使用BeautifulSoup

	使用BeautifulSoup通过属性查找标签的方法
		findAll()的使用：锁定标签类型--->锁定属性名--->锁定属性值
		findAll（“标签名”，{“属性名”：“属性值”}）  官方写法为：findAll（tagName，tagAttributes，recursive，text，limit，keywords）
		但是在95%的时间里都只需要前两个参数。详情请查询p33
		find()的使用：同findAll（），但是最后得到的是一个标签而不是一个标签组。
	番外篇：get_text()函数的使用
		谨慎使用get_text(),因为此函数会将所有的标签都清除掉。
		使用findAll（）函数得到一堆的标签时，我们并没有得到我们最终想要的数据
		此时使用  标签.get_text()  我们将得到标签中的文本信息，而将标签信息清除。
	使用BeautifulSoup中的导航树通过标签在文档中的位置来查找标签
		思想：BeautifulSoup在解析HTML时将一个标签的所有与他存在关系的
		标签划分为四种后代标签，子标签，兄弟标签，父标签。通过一个标签的标签关系得到的
		标签便是通过标签在文档中的位置来查找标签。
		一个HTML文件可以映射成一棵树，HTML标签就是根，body，head就是第一层子节点。
		使用子标签与后代标签：标签.children   得到标签的所有子标签
				标签.descendants()得到标签的所有子标签，与子标签的子标签。。。
		使用兄弟标签：标签.next_siblings   这个函数只调用标签后面的兄弟标签
			       标签.previous_siblings   这个函数只会调用标签前面的兄弟标签。
		使用父标签：标签.parent   这个函数会返回父标签。   父标签使用非常少。
******************************************************8.14正则表达式的使用***************************************************
上一节使用了findAll()函数，find（）函数，导航树来完成标签的
使用正则表达式是为了完成文本内容的筛选。
1.看懂正则表达式
	正则表达式中的每个字符之间都是且的关系。
	适配符与它前面的一个字符或者第一个括号组合使用。
	十二个适配符的含义：
		（）：括号内的内容为一个整体。
		^：指字符串开始位置的东西。^a：字符串的开头必须是a。
		$：出现在正则表达式的末尾，表示匹配动作由字符串的末端开始，即先查看字符串末端的字符是否符合正则表达式的规定。
		.:匹配任意单个字符。
		{m,n}:匹配前面的东西m到n次。
		[]:匹配[]中的任意一个字符。
		+：匹配前面的东西至少一次。
		*：匹配前面的东西0次或多次。
		
		[^]:匹配任意一个不在[^]中的字符。如[^A-C]，除了A，B，C都可以匹配。
		|：匹配任意一个由竖线分割的东西。如（a|b|c）。

		\（反斜杠）：转义字符，将特殊含义的字符转换成字面形式。
		?!：表示“不包含”的意思。使用时将?!放在一个东西前面，表示字符串中不能出现这个东西。

		
		





		
		
	


